{"cells":[{"cell_type":"code","execution_count":null,"id":"59e82865","metadata":{"id":"59e82865"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"f9d66a38","metadata":{"id":"f9d66a38","outputId":"d641e610-d18a-4f6e-c07b-97820bf62d39","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","print(device)"]},{"cell_type":"code","execution_count":null,"id":"ef9490c8","metadata":{"id":"ef9490c8","outputId":"4d27a2aa-84e6-47d4-bad7-5e1422819401"},"outputs":[{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#check gpu device (if using gpu)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n","File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:414\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n","File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n","File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["#check gpu device (if using gpu)\n","print(torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"id":"29a0c44a","metadata":{"id":"29a0c44a"},"outputs":[],"source":["#역번호가 199보다 작거나 1000보다 큰 역 데이터 제거 후 데이터프레임 재구성\n","df = pd.read_csv(\"2022_final.csv\", encoding='cp949')\n","stations_to_remove = df[(df['역번호'] > 1000) | (df['역번호'] < 199)].index\n","df.drop(stations_to_remove, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"487dbd09","metadata":{"id":"487dbd09"},"outputs":[],"source":["#MinMaxScaler 이용하여 데이터 스케일링하기\n","from sklearn.preprocessing import MinMaxScaler\n","\n","feature_scaler = MinMaxScaler(feature_range=(0, 1))\n","up_weekday_scaler = MinMaxScaler(feature_range=(0, 1))\n","up_saturday_scaler = MinMaxScaler(feature_range=(0, 1))\n","up_sunday_scaler = MinMaxScaler(feature_range=(0, 1))\n","down_weekday_scaler = MinMaxScaler(feature_range=(0, 1))\n","down_saturday_scaler = MinMaxScaler(feature_range=(0, 1))\n","down_sunday_scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","features = ['승차_Weekday', '승차_Saturday', '승차_Sunday', '하차_Weekday', '하차_Saturday', '하차_Sunday', '환승_Weekday', '환승_Saturday', '환승_Sunday', 'interval_Weekday', 'interval_Saturday', 'interval_Sunday', 'capacity']\n","\n","df[features] = feature_scaler.fit_transform(df[features])\n","df['상선_Weekday'] = up_weekday_scaler.fit_transform(df['상선_Weekday'].to_frame())\n","df['상선_Saturday'] = up_saturday_scaler.fit_transform(df['상선_Saturday'].to_frame())\n","df['상선_Sunday'] = up_sunday_scaler.fit_transform(df['상선_Sunday'].to_frame())\n","df['하선_Weekday'] = down_weekday_scaler.fit_transform(df['하선_Weekday'].to_frame())\n","df['하선_Saturday'] = down_saturday_scaler.fit_transform(df['하선_Saturday'].to_frame())\n","df['하선_Sunday'] = down_sunday_scaler.fit_transform(df['하선_Sunday'].to_frame())\n","\n","df.to_csv('2022_scaled.csv', index=False, encoding='cp949')"]},{"cell_type":"code","execution_count":null,"id":"dd845083","metadata":{"id":"dd845083","scrolled":true},"outputs":[],"source":["#'progression'열 새로 생성\n","df['progression'] = [0.0] * len(df)\n","\n","#각 노선 상/하행 데이터 시간대별로 분리하여 정리한 후 csv파일로 저장(평일)\n","weekday_up = ['역번호', '승차_Weekday', '하차_Weekday', '환승_Weekday', 'interval_Weekday', 'capacity', 'progression', '상선_Weekday']\n","weekday_down = ['역번호', '승차_Weekday', '하차_Weekday', '환승_Weekday', 'interval_Weekday', 'capacity', 'progression', '하선_Weekday']\n","weekday_up2 = ['승차_Weekday', '하차_Weekday', '환승_Weekday', 'interval_Weekday', 'capacity', 'progression', '상선_Weekday']\n","weekday_down2 = ['승차_Weekday', '하차_Weekday', '환승_Weekday', 'interval_Weekday', 'capacity', 'progression', '하선_Weekday']\n","hours = ['06-07시간대', '07-08시간대', '08-09시간대', '09-10시간대', '10-11시간대', '11-12시간대', '12-13시간대', '13-14시간대', '14-15시간대', '15-16시간대', '16-17시간대', '17-18시간대', '18-19시간대', '19-20시간대', '20-21시간대', '21-22시간대', '22-23시간대', '23-24시간대']\n","\n","start = [0, 0, 0, 9, 5, 10, 10, 9, 10]\n","end = [0, 0, 0, 52, 34, 48, 47, 50, 27]\n","num_stations = [0, 0, 43, 44, 51, 56, 39, 53, 18]\n","\n","for line in range(2,9):\n","    for period in hours:\n","        tmp = df.loc[df['호선'] == line]\n","        tmp = tmp[weekday_up]\n","        tmp2 = tmp.loc[df['hour'] == period]\n","        tmp2 = tmp2.sort_values(by='역번호', axis=0, ascending=False, inplace=False)\n","        if (line == 2):\n","            tmp2['progression'] = [0.5] * len(tmp2)\n","        else:\n","            tmp2['progression'] = (num_stations[line] - (tmp2['역번호'] - line * 100 - start[line])) / num_stations[line]\n","        tmp3 = tmp2[weekday_up2]\n","        pd.DataFrame(tmp3).to_csv(f'weekday_split\\\\{line}_{period}_up.csv', index=False, encoding='cp949')\n","\n","    for period in hours:\n","        tmp = df.loc[df['호선'] == line]\n","        tmp = tmp[weekday_down]\n","        tmp2 = tmp.loc[df['hour'] == period]\n","        tmp2 = tmp2.sort_values(by='역번호', axis=0, ascending=True, inplace=False)\n","        if (line == 2):\n","            tmp2['progression'] = [0.5] * len(tmp2)\n","        else:\n","            tmp2['progression'] = (tmp2['역번호'] - line * 100 - start[line]) / num_stations[line]\n","        tmp3 = tmp2[weekday_down2]\n","        pd.DataFrame(tmp3).to_csv(f'weekday_split\\\\{line}_{period}_down.csv', index=False, encoding='cp949')"]},{"cell_type":"code","execution_count":null,"id":"ac0fd0ce","metadata":{"id":"ac0fd0ce"},"outputs":[],"source":["#각 노선 상/하행 데이터 시간대별로 분리하여 정리한 후 csv파일로 저장(토요일)\n","saturday_up = ['역번호', '승차_Saturday', '하차_Saturday', '환승_Saturday', 'interval_Saturday', 'capacity', 'progression', '상선_Saturday']\n","saturday_down = ['역번호', '승차_Saturday', '하차_Saturday', '환승_Saturday', 'interval_Saturday', 'capacity', 'progression', '하선_Saturday']\n","saturday_up2 = ['승차_Saturday', '하차_Saturday', '환승_Saturday', 'interval_Saturday', 'capacity', 'progression', '상선_Saturday']\n","saturday_down2 = ['승차_Saturday', '하차_Saturday', '환승_Saturday', 'interval_Saturday', 'capacity', 'progression', '하선_Saturday']\n","\n","for line in range(2,9):\n","    for period in hours:\n","        tmp = df.loc[df['호선'] == line]\n","        tmp = tmp[saturday_up]\n","        tmp2 = tmp.loc[df['hour'] == period]\n","        tmp2 = tmp2.sort_values(by='역번호', axis=0, ascending=True, inplace=False)\n","        if (line == 2):\n","            tmp2['progression'] = [0.5] * len(tmp2)\n","        else:\n","            tmp2['progression'] = (num_stations[line] - (tmp2['역번호'] - line * 100 - start[line])) / num_stations[line]\n","        tmp3 = tmp2[saturday_up2]\n","        pd.DataFrame(tmp3).to_csv(f'saturday_split\\\\{line}_{period}_up.csv', index=False, encoding='cp949')\n","\n","    for period in hours:\n","        tmp = df.loc[df['호선'] == line]\n","        tmp = tmp[saturday_down]\n","        tmp2 = tmp.loc[df['hour'] == period]\n","        tmp2 = tmp2.sort_values(by='역번호', axis=0, ascending=False, inplace=False)\n","        if (line == 2):\n","            tmp2['progression'] = [0.5] * len(tmp2)\n","        else:\n","            tmp2['progression'] = (num_stations[line] - (tmp2['역번호'] - line * 100 - start[line])) / num_stations[line]\n","        tmp3 = tmp2[saturday_down2]\n","        pd.DataFrame(tmp3).to_csv(f'saturday_split\\\\{line}_{period}_down.csv', index=False, encoding='cp949')"]},{"cell_type":"code","execution_count":null,"id":"40a63d18","metadata":{"id":"40a63d18"},"outputs":[],"source":["#각 노선 상/하행 데이터 시간대별로 분리하여 정리한 후 csv파일로 저장(일요일)\n","sunday_up = ['역번호', '승차_Sunday', '하차_Sunday', '환승_Sunday', 'interval_Sunday', 'capacity', 'progression', '상선_Sunday']\n","sunday_down = ['역번호', '승차_Sunday', '하차_Sunday', '환승_Sunday', 'interval_Sunday', 'capacity', 'progression', '하선_Sunday']\n","sunday_up2 = ['승차_Sunday', '하차_Sunday', '환승_Sunday', 'interval_Sunday', 'capacity', 'progression', '상선_Sunday']\n","sunday_down2 = ['승차_Sunday', '하차_Sunday', '환승_Sunday', 'interval_Sunday', 'capacity', 'progression', '하선_Sunday']\n","\n","for line in range(2, 9):\n","    for period in hours:\n","        tmp = df.loc[df['호선'] == line]\n","        tmp = tmp[sunday_up]\n","        tmp2 = tmp.loc[df['hour'] == period]\n","        tmp2 = tmp2.sort_values(by='역번호', axis=0, ascending=True, inplace=False)\n","        if (line == 2):\n","            tmp2['progression'] = [0.5] * len(tmp2)\n","        else:\n","            tmp2['progression'] = (num_stations[line] - (tmp2['역번호'] - line * 100 - start[line])) / num_stations[line]\n","        tmp3 = tmp2[sunday_up2]\n","        pd.DataFrame(tmp3).to_csv(f'sunday_split\\\\{line}_{period}_up.csv', index=False, encoding='cp949')\n","\n","    for period in hours:\n","        tmp = df.loc[df['호선'] == line]\n","        tmp = tmp[sunday_down]\n","        tmp2 = tmp.loc[df['hour'] == period]\n","        tmp2 = tmp2.sort_values(by='역번호', axis=0, ascending=False, inplace=False)\n","        if (line == 2):\n","            tmp2['progression'] = [0.5] * len(tmp2)\n","        else:\n","            tmp2['progression'] = (num_stations[line] - (tmp2['역번호'] - line * 100 - start[line])) / num_stations[line]\n","        tmp3 = tmp2[sunday_down2]\n","        pd.DataFrame(tmp3).to_csv(f'sunday_split\\\\{line}_{period}_down.csv', index=False, encoding='cp949')"]},{"cell_type":"code","execution_count":null,"id":"13afb912","metadata":{"id":"13afb912"},"outputs":[],"source":["import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Training data (multiple time series)\n","import os\n","directories = ['weekday_split', 'saturday_split', 'sunday_split']\n","time_series_list = []\n","X = []\n","y = []\n","time_steps = 8\n","\n","for directory in directories:\n","    for filename in os.listdir(directory):\n","        f = os.path.join(directory, filename)\n","        if os.path.isfile(f):\n","            tmp = pd.read_csv(f, encoding='cp949')\n","            tmp = tmp.astype(float)\n","            tens = torch.from_numpy(tmp.values)\n","            time_series_list.append(tens)\n","\n","for ts in time_series_list:\n","    for i in range(len(ts) - time_steps):\n","        X.append(ts[i:i + time_steps])\n","        y.append(ts[i + time_steps, -1])  #the last feature is the target\n","\n","y = np.array(y)\n","# Convert to PyTorch tensors\n","X = torch.stack(X, dim=0)\n","X = torch.tensor(X, dtype=torch.float32)\n","y = torch.tensor(y, dtype=torch.float32)\n","print(X.size())\n","print(y.size())"]},{"cell_type":"code","execution_count":null,"id":"9b6e9fb3","metadata":{"id":"9b6e9fb3"},"outputs":[],"source":["class TimeSeriesDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx].unsqueeze(-1)\n","\n","dataset = TimeSeriesDataset(X, y)\n","\n","# Split the dataset into training and validation sets\n","train_size = int(0.8 * len(dataset))  # 80% of the data for training\n","val_size = len(dataset) - train_size  # Remaining 20% for validation\n","\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"id":"bd2532f8","metadata":{"id":"bd2532f8"},"outputs":[],"source":["#모델학습에 사용할 수 있도록 데이터셋 준비하기\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n","        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n","\n","        out, _ = self.lstm(x, (h_0, c_0))\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","input_size = X.shape[2] # Number of features in the input data\n","hidden_size = 60        # Number of features in the hidden state\n","num_layers = 4          # Number of stacked LSTM layers\n","output_size = 1         # Number of output features (1 target feature)\n","\n","model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"id":"83e3f13f","metadata":{"id":"83e3f13f","scrolled":true},"outputs":[],"source":["# Training loop\n","num_epochs = 70\n","for epoch in range(num_epochs):\n","    model.train()\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            val_loss += criterion(outputs, targets).item()\n","\n","    val_loss /= len(val_loader)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"715f9ea6","metadata":{"id":"715f9ea6"},"outputs":[],"source":["# New time series for testing\n","scaled_new_ts = pd.read_csv(\"4_20-21시간대_up.csv\", encoding='cp949')\n","scaled_new_ts = scaled_new_ts.to_numpy()\n","print(scaled_new_ts.shape)"]},{"cell_type":"code","execution_count":null,"id":"a2268951","metadata":{"id":"a2268951"},"outputs":[],"source":["actual = scaled_new_ts[:, -1].copy()\n","scaled_new_ts[time_steps:, -1] = 0.0 # Make target feature to zero (for prediction)\n","X_test = scaled_new_ts.copy()\n","# Initialize the placeholder for predictions\n","predictions = []"]},{"cell_type":"code","execution_count":null,"id":"82c549b7","metadata":{"id":"82c549b7"},"outputs":[],"source":["# Make predictions\n","model.eval()\n","with torch.no_grad():\n","    for t in range(time_steps, X_test.shape[0]+1):\n","        # Prepare the input for the model\n","        X_input = X_test[t-time_steps:t, :]  # First (time_steps) inputs are given to model\n","        X_input = [X_input]\n","\n","        # Convert to tensor\n","        X_input_tensor = torch.tensor(X_input, dtype=torch.float32).to(device)\n","\n","        # Predict the target feature\n","        y_pred = model(X_input_tensor)\n","        pred_value = y_pred.cpu().numpy()[0][0]\n","        # Update the placeholder with the predictions\n","        predictions.append(pred_value)\n","\n","        # Update the input with the predicted target feature for the next step\n","        if (t != X_test.shape[0]):\n","            X_test[t, -1] = pred_value\n","\n","predicted = predictions[:-1]\n","actual = actual[time_steps:]\n","# Turn back to original value\n","predicted = np.reshape(predicted, (-1,1))\n","predicted = down_saturday_scaler.inverse_transform(predicted)  # Switch scaler as needed\n","actual = np.reshape(actual, (-1,1))\n","actual = down_saturday_scaler.inverse_transform(actual)\n","\n","# Print the final prediction values of the target feature\n","print(\"Predicted values:\", predicted)\n","print(\"Actual values:\", actual)"]},{"cell_type":"code","execution_count":null,"id":"6080d68a","metadata":{"id":"6080d68a","scrolled":true},"outputs":[],"source":["#예측값과 실제값 비교 시각화\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(actual, label='Actual')\n","plt.plot(predicted, label='Predicted')\n","plt.title('Actual vs Predicted Values')\n","plt.xlabel('Time Step')\n","plt.ylabel('Value')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"dfbb5947-6c69-4afc-900a-50ae619b392a","metadata":{"id":"dfbb5947-6c69-4afc-900a-50ae619b392a"},"outputs":[],"source":["# 예측 값 및 실제 값\n","predicted = np.array(predicted)\n","actual = np.array(actual)\n","\n","# MSE (Mean Squared Error) 계산\n","mse = np.mean((predicted - actual) ** 2)\n","print(f'MSE: {mse:.4f}')\n","\n","# R^2 점수 계산\n","ss_total = np.sum((actual - np.mean(actual)) ** 2)\n","ss_residual = np.sum((actual - predicted) ** 2)\n","r2_score = 1 - (ss_residual / ss_total)\n","print(f'R^2 Score: {r2_score:.4f}')\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":5}
